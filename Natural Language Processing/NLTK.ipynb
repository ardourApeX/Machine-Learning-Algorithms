{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories = 'adventure' )\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "\n",
    "- Get the Corpus\n",
    "- Tokenisation, StopWard Removal\n",
    "- Stemming\n",
    "- Building a Vocal\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Science can amuse and fascinate us all, but it is engineering that changes the world.\n",
    "              The engineer has been, and is, a maker of history. \n",
    "              Scientists study the world as it is; engineers create the world that has never been.\n",
    "              The way to succeed is to double your failure rate.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents and other data at jamesbond@001.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize #one for conversion into token of sentences and another one for word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Science can amuse and fascinate us all, but it is engineering that changes the world.', 'The engineer has been, and is, a maker of history.', 'Scientists study the world as it is; engineers create the world that has never been.', 'The way to succeed is to double your failure rate.']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'and', 'other', 'data', 'at', 'jamesbond@001.com']\n"
     ]
    }
   ],
   "source": [
    "print(sentence.split(sep = \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'and', 'other', 'data', 'at', 'jamesbond', '@', '001.com']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #Some pre-defined non useful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sw = set(stopwords.words(\"english\")) #Stopwords in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with', 'isn', \"isn't\", 'ourselves', 'under', 'above', \"needn't\", 'up', 'all', 'wouldn', 'didn', 'how', 'very', 'hers', \"haven't\", \"wouldn't\", 'was', 'which', 'your', 'them', \"hadn't\", 'on', 'mustn', 'each', 'whom', 'doesn', 'some', 'couldn', 'myself', 'herself', 'before', 'to', 'if', 'by', 't', 'had', 're', 'did', 'can', \"wasn't\", 'and', 'against', 'wasn', 'yourself', 'into', \"that'll\", 'my', 'him', \"aren't\", \"mightn't\", \"should've\", 'now', 'having', 'from', 'weren', 'both', \"weren't\", 'm', 'while', \"hasn't\", 'these', 'we', 'those', \"don't\", 'in', \"didn't\", 'is', 'after', 'this', 'he', 'theirs', 'over', 'her', 'most', 'll', 'as', 'further', 'don', \"shan't\", 'won', 'y', \"mustn't\", \"won't\", 'for', 'when', 'themselves', 'only', 'here', 'doing', 'yours', 'be', 'why', \"you'd\", \"you're\", 'because', 'of', 'once', 'no', 'just', 'who', \"it's\", 'hasn', 'their', 'will', 'our', 've', 'his', 'were', \"shouldn't\", 'shan', 'what', 'such', 'the', 'are', 'again', 'needn', 'am', 'shouldn', 'they', 'nor', 'same', 'been', 'own', 'any', 'aren', 'being', 'its', 'himself', 'there', 'has', \"she's\", 'ours', \"couldn't\", 'between', 'more', 'down', 'not', 'd', \"you'll\", 'then', 'she', 'but', 'ma', 'off', 'where', 'me', 'do', 'too', 'o', 'yourselves', 's', 'haven', 'during', 'it', 'through', 'you', 'itself', \"you've\", 'at', 'ain', 'hadn', 'does', 'an', 'a', 'about', 'out', 'few', 'so', 'have', 'until', 'other', 'or', 'i', 'below', 'than', 'mightn', \"doesn't\", 'that', 'should'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(eng_sw)\n",
    "print(len(eng_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(text, StopWords):\n",
    "    useful_Words = [word  for word in text if word not in StopWords]\n",
    "    return useful_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "demo_text = \"i am not bothered about her very much\".split()\n",
    "useful_text = RemoveStopWords(demo_text, eng_sw)\n",
    "print(useful_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
